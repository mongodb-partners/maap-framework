import 'dotenv/config';
import {
    getModelClass,
    getEmbeddingModel,
    getVBDConfigInfo,
    getAggregateOperatorConfigs,
    getSystemPrompt
} from '../../../src/yaml_parser/src/LoadYaml.js';
import {
    BaseModel,
    PreProcessQuery,
    RAGApplicationBuilder,
    Rerank,
    convertBaseEmbeddingsToEmbedder,
    convertBaseModelToChatLlm,
    withQueryPreprocessor,
    withReranker,
} from '../../../src/index.js';

import { MongoClient } from 'mongodb';
import {
    makeDefaultFindContent,
    MakeUserMessageFunc,
    OpenAiChatMessage,
    GenerateUserPromptFunc,
    makeRagGenerateUserPrompt,
    SystemPrompt,
    makeMongoDbConversationsService,
    AppConfig,
    makeApp,
} from 'mongodb-chatbot-server';
import { makeMongoDbEmbeddedContentStore, logger, EmbeddedContent, EmbeddedContentStore, WithScore } from 'mongodb-rag-core';
import { MongoDBCrud } from '../../../src/db/mongodb-crud.js';
import { AggMqlOperator } from '../../../src/db/dynamic-agg-operator.js';


// Load MAAP base classes
const model = getModelClass();
const embedding_model = getEmbeddingModel();
const { dbName, connectionString, vectorSearchIndexName, minScore, numCandidates } = getVBDConfigInfo();
const mongodb = new MongoClient(connectionString);

// Load crud operator with query and name of the operator
const crudOperatorConfigs = getAggregateOperatorConfigs();
var aggregatorPipelines = [];
if (crudOperatorConfigs) {
    for (const crudConfig of crudOperatorConfigs) {
        const crud = new MongoDBCrud({ connectionString: crudConfig.connectionString, dbName: crudConfig.dbName, collectionName: crudConfig.collectionName });
        crud.init();
        const aggQueryTemplate = JSON.parse(crudConfig.query);
        aggregatorPipelines.push({ crudOperator: crud, aggregator: new AggMqlOperator({ model: model, queryTemplate: aggQueryTemplate, jsonSchema: crudConfig.jsonSchema }) });
    }
}

// Asynchronously generates a structured query context based on the original user message.
async function getStructuredQueryContext(originalUserMessage: string, recall?: boolean, p0?: boolean) {
    var structuredQueryContext = "";
    for (const aggregatorPipeline of aggregatorPipelines) {
        recall = recall ?? false;
        const query = await aggregatorPipeline.aggregator.runQuery(originalUserMessage, recall);
        // if query is null do not append to structuredQueryContext
        if (query) {
            aggregatorPipeline.crudOperator.init();
            const result = await aggregatorPipeline.crudOperator.aggregate(query);
            structuredQueryContext = structuredQueryContext + "\n" + JSON.stringify(result);
        }
    }
    return structuredQueryContext;
}

// Asynchronously fetches the conversation history from the MongoDB database.
const getConversationHistory = async () => {
    var conversationHistory = "";
    try {
        const latestMessage = await mongodb.db(dbName).collection('conversations').findOne({}, { sort: { _id: -1 }, projection: { _id: 0, messages: 1 } });
        if (latestMessage) {
            latestMessage.messages.forEach((message: { role: string; content: string }) => {
                conversationHistory += `${message.role} : ${message.content} ~~~ `;
            });
        }
    } catch (error) {
        logger.error(`Error fetching conversation history: ${error}`);
    }
    return conversationHistory;
}

// MongoDB data source for the content used in RAG.
// Generated with the Ingest CLI.
const embeddedContentStore = makeMongoDbEmbeddedContentStore({
    connectionUri: connectionString,
    databaseName: dbName,
});

// Convert MAAP base embeddings to framework's Embedder
// console.log(embedding_model)
const embedder = convertBaseEmbeddingsToEmbedder(embedding_model);

// Convert MAAP base LLM to framework's ChatLlm
const llm = await convertBaseModelToChatLlm(model);


// Find content in the embeddedContentStore using the vector embeddings
// generated by the embedder.
const findContent = makeDefaultFindContent({
    embedder,
    store: embeddedContentStore,
    findNearestNeighborsOptions: {
        k: 2,
        path: 'embedding',
        indexName: vectorSearchIndexName,
        numCandidates: numCandidates,
        minScore: minScore,
    }
});

// For MAAP team: this shows how to use the withReranker and withQueryPreprocessor
// functions to wrap the findContent function with reranking and preprocessing functionality.
const dummyRerank: Rerank = async ({ query, results }) => {
    return { results };
};
const dummyPreprocess: PreProcessQuery = async ({ query }) => {
    // Aggreation query result + User query
    const preprocessedQuery = await getStructuredQueryContext(query, false);
    return { preprocessedQuery: preprocessedQuery };
};
const findContentWithRerank = withReranker({ findContentFunc: findContent, reranker: dummyRerank });
var findContentWithPreprocess = withQueryPreprocessor({
    findContentFunc: findContent,
    queryPreprocessor: dummyPreprocess,
});
if (aggregatorPipelines.length < 1) {
    findContentWithPreprocess = findContent;
}

// Constructs the user message sent to the LLM from the initial user message
// and the content found by the findContent function.
const makeUserMessage: MakeUserMessageFunc = async function ({
    content,
    originalUserMessage,
}): Promise<OpenAiChatMessage & { role: 'user' }> {
    const chunkSeparator = '~~~~~~';
    const context = content.map((c) => c.text).join(`\n${chunkSeparator}\n`);
    // Run the structured query to populate the alternate retrieval technique results as context
    var structuredQueryContext = "";
    if (aggregatorPipelines.length > 0) {
        var structuredQueryContext = await getStructuredQueryContext(originalUserMessage, true);
    }
    const conversationHistory = await getConversationHistory();

    const contentForLlm = `Using the following information, answer the user query.
    the context is separated by Chunk Separator: ${chunkSeparator}

    Information:
    ${context}    
    ${chunkSeparator}
    Conversation History:
    ${conversationHistory}
    Operational Information:
    ${structuredQueryContext}

    User query: ${originalUserMessage}`;
    return { role: 'user', content: contentForLlm };
};

// Generates the user prompt for the chatbot using RAG
const generateUserPrompt: GenerateUserPromptFunc = makeRagGenerateUserPrompt({
    // findContent: findContentWithRerankAndPreprocess,
    findContent: findContentWithPreprocess,
    makeUserMessage,
});

// System prompt for chatbot
const systemPrompt: SystemPrompt = {
    role: 'system',
    content: getSystemPrompt(),
};

// Create MongoDB collection and service for storing user conversations
// with the chatbot.

const conversations = makeMongoDbConversationsService(mongodb.db(dbName));

// Create the MongoDB Chatbot Server Express.js app configuration
const config: AppConfig = {
    conversationsRouterConfig: {
        llm,
        conversations,
        generateUserPrompt,
        systemPrompt,
    },
    maxRequestTimeoutMs: 30000,
    serveStaticSite: true,
};

// Start the server and clean up resources on SIGINT.
const PORT = process.env.PORT || 9000;
const startServer = async () => {
    logger.info('Starting server...');
    const app = await makeApp(config);
    const server = app.listen(PORT, () => {
        logger.info(`Server listening on port: ${PORT}`);
    });

    process.on('SIGINT', async () => {
        logger.info('SIGINT signal received');
        await mongodb.close();
        await embeddedContentStore.close();
        await new Promise<void>((resolve, reject) => {
            server.close((error: any) => {
                error ? reject(error) : resolve();
            });
        });
        process.exit(1);
    });
};

try {
    startServer();
} catch (e) {
    logger.error(`Fatal error: ${e}`);
    process.exit(1);
}

