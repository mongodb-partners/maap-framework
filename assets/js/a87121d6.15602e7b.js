"use strict";(self.webpackChunkmaap_docs=self.webpackChunkmaap_docs||[]).push([[317],{5515:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});var o=t(4848),a=t(8453);const r={},s="Real Time Data Loader",i={id:"framework/app-modules/data-ingestion/real-time-data-loader",title:"Real Time Data Loader",description:"In this document you'll find a guide to configure a kafka environment with the respective",source:"@site/docs/framework/app-modules/data-ingestion/real-time-data-loader.md",sourceDirName:"framework/app-modules/data-ingestion",slug:"/framework/app-modules/data-ingestion/real-time-data-loader",permalink:"/maap-framework/docs/framework/app-modules/data-ingestion/real-time-data-loader",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Enterprise Data Loader (Using unstructured)",permalink:"/maap-framework/docs/framework/app-modules/data-ingestion/enterise-data-loader"}},c={},l=[{value:"Pre-requisites",id:"pre-requisites",level:2},{value:"Kafka Setup",id:"kafka-setup",level:2},{value:"Step by step guide",id:"step-by-step-guide",level:3},{value:"For Linux",id:"for-linux",level:4},{value:"For Windows",id:"for-windows",level:4},{value:"Data Loader",id:"data-loader",level:2},{value:"Test",id:"test",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"real-time-data-loader",children:"Real Time Data Loader"}),"\n",(0,o.jsx)(n.p,{children:"In this document you'll find a guide to configure a kafka environment with the respective\nmongo db connectors and all you need to run a real time data loader in your local environment."}),"\n",(0,o.jsx)(n.h1,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#pre-requisites",children:"Pre-requisites"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#kafka-setup",children:"Kafka Setup"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#data-loader",children:"Data Loader Usage"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"#test",children:"Test Example"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"pre-requisites",children:"Pre-requisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Docker installed with docker-compose command enabled."}),"\n",(0,o.jsx)(n.li,{children:"Atlas Cluster running with the proper databases and collections."}),"\n",(0,o.jsx)(n.li,{children:"A user with readWrite and changeStream roles for the Atlas Cluster."}),"\n",(0,o.jsx)(n.li,{children:"In Windows you'll need a git bash or some other Linux-based shell to run the script."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"kafka-setup",children:"Kafka Setup"}),"\n",(0,o.jsx)(n.p,{children:"If you already have a kafka service running you can ignore this section."}),"\n",(0,o.jsx)(n.h3,{id:"step-by-step-guide",children:"Step by step guide"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Locate the start-kafka.sh script at the scripts folder in this repo."}),"\n",(0,o.jsx)(n.li,{children:"Run the run.sh script with the respective parameters in your shell"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:' ./start-kafka.sh --mongo-uri "mongodb+srv://<username:password>@<yourcluster>.mongodb.net/" --database "<your-database>" --collection "<your-collection>" --kafka-bootstrap-servers "localhost:9092" --sink_topic "<sink_topic_name>" --source_topic "<source_topic_name>"\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OPTIONAL"}),": Configure your own connectors. You can create additional connectors to monitor other collections (source)\nor send data to your database from other topics (sink)."]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"for-linux",children:"For Linux"}),"\n",(0,o.jsx)(n.p,{children:"Note: The topics for those connector must be different. Example: userAvailable for sink connector\nand intermediateTopic for source connector. The topic used at the source connector is the one that the\nRTDL is going to use to listen to messages."}),"\n",(0,o.jsxs)(n.p,{children:["Sink Connector. Configure ",(0,o.jsx)(n.code,{children:"<your_atlas_uri>"}),", ",(0,o.jsx)(n.code,{children:"<your_database>"}),", ",(0,o.jsx)(n.code,{children:"<your_collection>"}),", ",(0,o.jsx)(n.code,{children:"<your_topic>"})," and\nthe kafka.bootstrap.servers if you're using a different kafka deployment."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Content-Type: application/json" --data \'\n{\n   "name": "mongodb-sink-connector",\n   "config": {\n   "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",\n   "connection.uri": "<your_atlas_uri>",\n   "database": "<your_database>",\n   "collection": "<your_collection>",\n   "topics": "<your_topic>",\n   "key.converter": "org.apache.kafka.connect.storage.StringConverter",\n   "value.converter.schemas.enable": false,\n   "value.converter": "org.apache.kafka.connect.json.JsonConverter",\n   "mongo.errors.tolerance": "data",\n   "errors.tolerance": "all",\n   "errors.deadletterqueue.topic.name": "mongoerror",\n   "errors.deadletterqueue.topic.replication.factor": 1,\n   "kafka.bootstrap.servers": "localhost:9092"\n}}\' http://localhost:8083/connectors -w "\\n"\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Source Connector. Configure ",(0,o.jsx)(n.code,{children:"<your_atlas_uri>"}),", ",(0,o.jsx)(n.code,{children:"<your_database>"}),", the topic.namespace.map following the pattern and\ndisplayed in the example and the kafka.bootstrap.servers if you're using a different kafka deployment."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:'curl -X POST -H "Content-Type: application/json" --data \'\n{\n    "name": "mongodb-maap-data-connector",\n    "config": {\n    "connector.class": "com.mongodb.kafka.connect.MongoSourceConnector",\n    "connection.uri": "<your_atlas_uri>",\n    "database": "<your_database>",\n    "topic.namespace.map": "{\\"<your_database>.<your_collection1>\\": \\"<your_topic1>\\", \\"<your_database>.<your_collection2>\\": \\"<your_topic2>\\", \\"<your_database>.<your_collectionN>\\": \\"<your_topicN>\\"}",\n    "copy.existing": true,\n    "poll.max.batch.size": 100,\n    "poll.await.time.ms": 5000,\n    "kafka.bootstrap.servers": "localhost:9092",\n    "tasks.max": "1",\n    "heartbeat.interval.ms": 3000,\n    "errors.tolerance": "all",\n    "errors.log.enable": true,\n    "errors.log.include.messages": true,\n    "auto.offset.reset": "latest",\n    "enable.auto.commit": true,\n    "acks": "all",\n    "pipeline": "[{\\"$match\\": {\\"operationType\\": {\\"$in\\": [\\"insert\\", \\"update\\", \\"replace\\"]}}}]"\n}}\' http://localhost:8083/connectors -w "\\n"\n'})}),"\n",(0,o.jsx)(n.h4,{id:"for-windows",children:"For Windows"}),"\n",(0,o.jsx)(n.p,{children:"Create 1 json file for each connector anywhere in your computer:"}),"\n",(0,o.jsxs)(n.p,{children:["Configure ",(0,o.jsx)(n.code,{children:"<your_atlas_uri>"}),", ",(0,o.jsx)(n.code,{children:"<your_database>"}),", ",(0,o.jsx)(n.code,{children:"<your_collection>"}),", ",(0,o.jsx)(n.code,{children:"<your_topic>"})," and the kafka.bootstrap.servers if you're using a different kafka deployment."]}),"\n",(0,o.jsx)(n.p,{children:"Note: The topics for those connector must be different. Example: userAvailable for sink connector\nand intermediateTopic for source connector. The topic used at the source connector is the one that the\nRTDL is going to use to listen to messages."}),"\n",(0,o.jsx)(n.p,{children:"Sink Connector"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n\t"name": "mongodb-sink-connector",\n\t"config": {\n\t\t"connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",\n\t\t"connection.uri": "<your_atlas_uri>",\n\t\t"database": "<your_database>",\n\t\t"collection": "<your_collection>",\n\t\t"topics": "<your_topics>",\n\t\t"key.converter": "org.apache.kafka.connect.storage.StringConverter",\n\t\t"value.converter.schemas.enable": false,\n\t\t"value.converter": "org.apache.kafka.connect.json.JsonConverter",\n\t\t"mongo.errors.tolerance": "data",\n\t\t"errors.tolerance": "all",\n\t\t"errors.deadletterqueue.topic.name": "mongoerror",\n\t\t"errors.deadletterqueue.topic.replication.factor": 1,\n\t\t"kafka.bootstrap.servers": "localhost:9092"\n\t}\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Source Connector"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n\t"name": "mongodb-maap-data-connector",\n\t"config": {\n\t\t"connector.class": "com.mongodb.kafka.connect.MongoSourceConnector",\n\t\t"connection.uri": "<your_atlas_uri>",\n\t\t"database": "<your_database>",\n\t\t"topic.namespace.map": "{\\"<your_database>.<your_collection1>\\": \\"<your_topic1>\\", \\"<your_database>.<your_collection2>\\": \\"<your_topic2>\\", \\"<your_database>.<your_collectionN>\\": \\"<your_topicN>\\"}",\n\t\t"copy.existing": true,\n\t\t"poll.max.batch.size": 100,\n\t\t"poll.await.time.ms": 5000,\n\t\t"kafka.bootstrap.servers": "localhost:9092",\n\t\t"tasks.max": "1",\n\t\t"heartbeat.interval.ms": 3000,\n\t\t"errors.tolerance": "all",\n\t\t"errors.log.enable": true,\n\t\t"errors.log.include.messages": true,\n\t\t"auto.offset.reset": "latest",\n\t\t"enable.auto.commit": true,\n\t\t"acks": "all",\n\t\t"pipeline": "[{\\"$match\\": {\\"operationType\\": {\\"$in\\": [\\"insert\\", \\"update\\", \\"replace\\"]}}}]"\n\t}\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Run the following command in powershell:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:'Invoke-WebRequest -Uri "http://localhost:8083/connectors" -Method Post -Headers @{ "Content-Type" = "application/json" } -InFile "path\\to\\your\\config_files.json"\n'})}),"\n",(0,o.jsx)(n.p,{children:"Note: the -Uri is pointing to your connectors endpoint, change it for the one pointing to your connectors endpoint."}),"\n",(0,o.jsx)(n.h2,{id:"data-loader",children:"Data Loader"}),"\n",(0,o.jsx)(n.p,{children:"This data loader is used to receive messages in real time from a Kafka topic. The ingest runs indefinitely until the process\nis stopped manually. Everytime a message is received, it looks for the content that is meant to be\nprocessed makes an entry into the configured database."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Usage:"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-js",children:"ingest:\n    - source: 'realtime'\n      topic: 'source-connector-topic'\n      brokers: ['localhost:9092']\n      fields: ['field1', 'field2', 'fieldN']\n      thumblingWindow: 6000\n      chunk_size: 600\n      chunk_overlap: 60\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Architecture:"}),"\n",(0,o.jsx)(n.img,{alt:"kafkaArchitectureRTDL.png",src:t(5663).A+"",width:"1132",height:"368"})]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Expected format for Sink topic messages"})}),"\n",(0,o.jsx)(n.p,{children:"The only required field is the content field, the rest of fields are going to be set as metadata for the metadata field in the main db."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{"content": "Plain text goes here. It could be loooong\u2026", "metadata1":"value1", "metadata2":"value2", "metadataN":"valueN"}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Example:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{"database_collection": "StreamData", "content": "Plain text goes here. It could be loooong\u2026", "title": "Test Text", "created": "2021-11-17T03:19:56.186Z"}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"test",children:"Test"}),"\n",(0,o.jsxs)(n.p,{children:["Download kafka in your computer local computer: ",(0,o.jsx)(n.a,{href:"https://kafka.apache.org/downloads",children:"https://kafka.apache.org/downloads"})]}),"\n",(0,o.jsx)(n.p,{children:"Unzip the folder content and go to the bin folder in a console.\nFor Linux run the .sh command, for Windows go one level deeper to the folder called windows and run the equivalent .bat"}),"\n",(0,o.jsx)(n.p,{children:"Run"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"./kafka-console-producer.sh --broker-list localhost:9092 --topic <your_sink_connector_topic>\n"})}),"\n",(0,o.jsx)(n.p,{children:"Send a message with the appropriate json format (described before)."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected behaviour"})," (as described in the architecture)"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The sink connector uploads the file inserted into the atlas database. (This works for any insertion with the proper format into this topic, not only from console)"}),"\n",(0,o.jsx)(n.li,{children:"The source connector detects that a file has been inserted into the collection. (This works with direct insertions too)."}),"\n",(0,o.jsx)(n.li,{children:"MAAP receives the message in the source topic and extracts the content."}),"\n",(0,o.jsx)(n.li,{children:"MAAP creates the embeddings and writes to the final database."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},5663:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/kafkaArchitectureRTDL-e38b153315a8461a12352c888aa90500.png"},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>i});var o=t(6540);const a={},r=o.createContext(a);function s(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);