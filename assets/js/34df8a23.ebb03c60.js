"use strict";(self.webpackChunkmaap_docs=self.webpackChunkmaap_docs||[]).push([[1788],{2757:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>s,contentTitle:()=>t,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"framework/partners/ollama","title":"Ollama","description":"Introduction","source":"@site/docs/framework/partners/ollama.md","sourceDirName":"framework/partners","slug":"/framework/partners/ollama","permalink":"/maap-framework/docs/framework/partners/ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI","permalink":"/maap-framework/docs/framework/partners/openai"},"next":{"title":"NVIDIA","permalink":"/maap-framework/docs/framework/partners/nvidia"}}');var o=l(4848),r=l(8453);const i={sidebar_position:12},t="Ollama",s={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Deploying your model",id:"deploying-your-model",level:2},{value:"Chat Model",id:"chat-model",level:3},{value:"Usage with MAAP",id:"usage-with-maap",level:4},{value:"Config File:",id:"config-file",level:4},{value:"Environment Variable :",id:"environment-variable-",level:4},{value:"Deploying your model using the LlamaIndex framework",id:"deploying-your-model-using-the-llamaindex-framework",level:2},{value:"Chat Model",id:"chat-model-1",level:3},{value:"Config File",id:"config-file-1",level:4},{value:"References",id:"references",level:3},{value:"Model Name",id:"model-name",level:5}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"ollama",children:"Ollama"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"})," is a powerful platform for running local AI models with enhanced privacy and control. It enables developers to deploy and customize chat completion models with ease."]}),"\n",(0,o.jsx)(n.h2,{id:"deploying-your-model",children:"Deploying your model"}),"\n",(0,o.jsxs)(n.p,{children:["Ollama supports deploying various Chat Models, including advanced LLMs tailored for local usage. For personal use you must create an account and ",(0,o.jsx)(n.a,{href:"https://www.ollama.com/download",children:"download"})," Ollama into your computer."]}),"\n",(0,o.jsx)(n.h3,{id:"chat-model",children:"Chat Model"}),"\n",(0,o.jsxs)(n.p,{children:["Refer to their ",(0,o.jsx)(n.a,{href:"https://www.ollama.com/blog",children:"blog"})," to understand the latest offerings and features."]}),"\n",(0,o.jsx)(n.h4,{id:"usage-with-maap",children:"Usage with MAAP"}),"\n",(0,o.jsx)(n.p,{children:"To use the Ollama chat model with the MAAP framework, you would need to feed the below values."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.h4,{id:"config-file",children:"Config File:"}),"\n",(0,o.jsxs)(n.p,{children:["Provided below are the values required to be added in the ",(0,o.jsx)(n.code,{children:"config.yaml"})," file in the LLM section."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"llms:\n  class_name: Ollama\n  model_name: <check_references_below>\n  base_url: <optional, defaults to http://localhost:11434>\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.h4,{id:"environment-variable-",children:"Environment Variable :"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"No additional environment variables are required, as Ollama operates locally by default."}),"\n",(0,o.jsx)(n.h2,{id:"deploying-your-model-using-the-llamaindex-framework",children:"Deploying your model using the LlamaIndex framework"}),"\n",(0,o.jsx)(n.h3,{id:"chat-model-1",children:"Chat Model"}),"\n",(0,o.jsx)(n.p,{children:"MAAP now provides the option to choose if you want to use LlamaIndex as your main framework to deploy your LLM models."}),"\n",(0,o.jsx)(n.p,{children:"This can be done by adding the 'framework' configuration to the config.yaml file"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.h4,{id:"config-file-1",children:"Config File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"llms:\n  class_name: Ollama\n  model_name: <check_references_below>\n  base_url: <optional, defaults to http://localhost:11434>\n  framework: llamaindex\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,o.jsx)(n.p,{children:"For more information on setting up and using Ollama, visit their documentation."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.h5,{id:"model-name",children:"Model Name"}),"\n","You can pick any model from the ",(0,o.jsx)(n.a,{href:"https://www.ollama.com/search",children:"updated list"})," given in the Ollama website."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,l)=>{l.d(n,{R:()=>i,x:()=>t});var a=l(6540);const o={},r=a.createContext(o);function i(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);